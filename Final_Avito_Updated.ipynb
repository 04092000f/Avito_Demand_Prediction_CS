{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Downloading the Dataset Stored in Google Drive\n",
        "!gdown --id 1gJl34v3SzTbWTtMs0uu2BgsFpnESLezx\n",
        "!gdown --id 1-7yi62S6FAMadRhNF6fvVL916lssHNSP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djh19lY9kj7L",
        "outputId": "a9990528-3fd4-4227-ff77-ec58e551ba58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gJl34v3SzTbWTtMs0uu2BgsFpnESLezx\n",
            "To: /content/Train_For_Pipeline.csv\n",
            "100% 1.09G/1.09G [00:09<00:00, 117MB/s] \n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-7yi62S6FAMadRhNF6fvVL916lssHNSP\n",
            "To: /content/Test_For_Pipeline.csv\n",
            "100% 379M/379M [00:04<00:00, 84.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from string import punctuation\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import argparse"
      ],
      "metadata": {
        "id": "YGjEhCKAv0J5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simplemma==0.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb8HGT5x__LH",
        "outputId": "40305c9d-c0ef-41a7-fb13-1b1bab8abdb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simplemma==0.5.0\n",
            "  Downloading simplemma-0.5.0-py3-none-any.whl (63.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.7 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: simplemma\n",
            "Successfully installed simplemma-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from nltk.stem import SnowballStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download('stopwords')\n",
        "### Dataset Preprocessing\n",
        "from simplemma import text_lemmatizer\n",
        "import simplemma\n",
        "langdata = simplemma.load_data('ru')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB61AgZL_-Ci",
        "outputId": "b8c5993b-d3b6-4315-f4e4-c770a5f7c0d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_df=pd.read_csv(\"Train_For_Pipeline.csv\")\n",
        "te_df=pd.read_csv(\"Test_For_Pipeline.csv\")"
      ],
      "metadata": {
        "id": "xMsa-D7qM66_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as kb\n",
        "def rmse(y_true,y_pred):\n",
        "  return kb.sqrt(kb.mean(kb.square(y_pred-y_true)))"
      ],
      "metadata": {
        "id": "W3qCtWTrt2ve"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_pred(data):\n",
        "  def load_sample(df):\n",
        "    df=df.sample(n=1,axis=0)\n",
        "    return df\n",
        "\n",
        "  def feature_engg(df):\n",
        "    df['activation_date'] = pd.to_datetime(df['activation_date'], errors = 'coerce')\n",
        "    df['price']=np.log(df['price']+0.001)\n",
        "    df['day'] = df['activation_date'].dt.day\n",
        "    df['dayofweek_name'] = df['activation_date'].dt.day_name\n",
        "    df['is_weekend'] = np.where(df['dayofweek_name'].isin(['Sunday','Saturday']),1,0)\n",
        "    df['weekday'] = df['activation_date'].dt.weekday\n",
        "    df['description_len'] = df['description'].apply(lambda x : len(x.split()))\n",
        "    df['title_len'] = df['title'].apply(lambda x : len(x.split()))\n",
        "    df['param_combined'] = df.apply(lambda row: ' '.join([str(row['param_1']), str(row['param_2']),  str(row['param_3'])]), axis=1)\n",
        "    df['param_combined_len'] = df['param_combined'].apply(lambda x : len(x.split()))\n",
        "    df['description_char'] = df['description'].apply(len)\n",
        "    df['title_char'] = df['title'].apply(len)\n",
        "    df['param_char'] = df['param_combined'].apply(len)\n",
        "    df['punctuation_count'] = df['description'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) \n",
        "    return df\n",
        "\n",
        "  def data_encoding(df):\n",
        "    region_enc=pickle.load(open(\"/content/Region_Encoder.pkl\",\"rb\"))\n",
        "    df['region_enc']=region_enc.transform(df['region'].values.reshape(-1,1))\n",
        "    city_enc=pickle.load(open(\"/content/City_Encoder.pkl\",\"rb\"))\n",
        "    df['city_enc']=city_enc.transform(df['city'].values.reshape(-1,1))\n",
        "    pcn_enc=pickle.load(open(\"/content/parent_category_name_Encoder.pkl\",\"rb\"))\n",
        "    df['pcn_enc']=pcn_enc.transform(df['parent_category_name'].values.reshape(-1,1))\n",
        "    cn_enc=pickle.load(open(\"/content/category_name_Encoder.pkl\",\"rb\"))\n",
        "    df['cn_enc']=cn_enc.transform(df['category_name'].values.reshape(-1,1))\n",
        "    ut_enc=pickle.load(open(\"/content/user_type_Encoder.pkl\",\"rb\"))\n",
        "    df['ut_enc']=ut_enc.transform(df['user_type'].values.reshape(-1,1))\n",
        "    p1_enc=pickle.load(open(\"/content/param_1_Encoder.pkl\",\"rb\"))\n",
        "    df['p1_enc']=p1_enc.transform(df['param_1'].values.reshape(-1,1))\n",
        "    p2_enc=pickle.load(open(\"/content/param_2_Encoder.pkl\",\"rb\"))\n",
        "    df['p2_enc']=p2_enc.transform(df['param_2'].values.reshape(-1,1))\n",
        "    p3_enc=pickle.load(open(\"/content/param_3_Encoder.pkl\",\"rb\"))\n",
        "    df['p3_enc']=p3_enc.transform(df['param_3'].values.reshape(-1,1))\n",
        "    # Numerical Features\n",
        "  \n",
        "    price_enc=pickle.load(open(\"/content/Scaled_price.pkl\",\"rb\"))\n",
        "    df['price_enc']=price_enc.transform(df['price'].values.reshape(-1,1))\n",
        "    des_len_enc=pickle.load(open(\"/content/Scaled_description_len.pkl\",\"rb\"))\n",
        "    df['des_len_enc']=des_len_enc.transform(df['description_len'].values.reshape(-1,1))\n",
        "    tit_len_enc=pickle.load(open(\"/content/Scaled_title_len.pkl\",\"rb\"))\n",
        "    df['tit_len_enc']=tit_len_enc.transform(df['title_len'].values.reshape(-1,1))\n",
        "    pc_len_enc=pickle.load(open(\"/content/Scaled_param_combined_len.pkl\",\"rb\"))\n",
        "    df['pc_len_enc']=pc_len_enc.transform(df['param_combined_len'].values.reshape(-1,1))\n",
        "    des_char_enc=pickle.load(open(\"/content/Scaled_description_char.pkl\",\"rb\"))\n",
        "    df['des_char_enc']=des_char_enc.transform(df['description_char'].values.reshape(-1,1))\n",
        "    tit_char_enc=pickle.load(open(\"/content/Scaled_title_char.pkl\",\"rb\"))\n",
        "    df['tit_char_enc']=tit_char_enc.transform(df['title_char'].values.reshape(-1,1))\n",
        "    param_char_enc=pickle.load(open(\"/content/Scaled_param_char.pkl\",\"rb\"))\n",
        "    df['param_char_enc']=param_char_enc.transform(df['param_char'].values.reshape(-1,1))\n",
        "    punc_enc=pickle.load(open(\"/content/Scaled_punctuation_count.pkl\",\"rb\"))\n",
        "    df['punc_enc']=punc_enc.transform(df['punctuation_count'].values.reshape(-1,1))\n",
        "    avg_red_enc=pickle.load(open(\"/content/Scaled_average_red.pkl\",\"rb\"))\n",
        "    df['avg_red_enc']=avg_red_enc.transform(df['average_red'].values.reshape(-1,1))\n",
        "    avg_green_enc=pickle.load(open(\"/content/Scaled_average_green.pkl\",\"rb\"))\n",
        "    df['avg_green_enc']=avg_green_enc.transform(df['average_green'].values.reshape(-1,1))\n",
        "    avg_green_enc=pickle.load(open(\"/content/Scaled_average_green.pkl\",\"rb\"))\n",
        "    df['avg_green_enc']=avg_green_enc.transform(df['average_green'].values.reshape(-1,1))\n",
        "    avg_blue_enc=pickle.load(open(\"/content/Scaled_average_blue.pkl\",\"rb\"))\n",
        "    df['avg_blue_enc']=avg_blue_enc.transform(df['average_blue'].values.reshape(-1,1))\n",
        "    blur_enc=pickle.load(open(\"/content/Scaled_img_blur.pkl\",\"rb\"))\n",
        "    df['blur_enc']=blur_enc.transform(df['img_blur'].values.reshape(-1,1))\n",
        "    return df\n",
        "\n",
        "  def text_preprocess(df,col):\n",
        "    corpus=[]\n",
        "    review=re.sub(r'[^\\w\\s]',\" \",df[col].values[0])\n",
        "    review=re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", review)\n",
        "    review = re.sub(r\"http\\S+\", \" \", review)\n",
        "    review = BeautifulSoup(review, 'lxml').get_text()\n",
        "    review = re.sub('_',' ',review)\n",
        "    review = ' '.join(text_lemmatizer(review, langdata))\n",
        "    review = ' '.join([word for word in review.split() if not word in stopwords.words('russian')])\n",
        "    review = ' '.join([word for word in review.split() if not word in (punctuation)])\n",
        "    review = review.lower()\n",
        "    corpus.append(review)\n",
        "    return corpus\n",
        "\n",
        "  def text_generate(corpus,voc_size,maxleng):\n",
        "    onehot_repr=[one_hot(words,voc_size)for words in corpus]\n",
        "    embedded_docs=pad_sequences(onehot_repr,padding='post',maxlen=maxleng)\n",
        "    return embedded_docs\n",
        "\n",
        "\n",
        "#'price_enc', 'des_len_enc', 'tit_len_enc', 'pc_len_enc', 'des_char_enc',\n",
        "#       'tit_char_enc', 'param_char_enc', 'punc_enc', 'avg_red_enc',\n",
        "#       'avg_green_enc', 'blur_enc'\n",
        "\n",
        "  def predict(df,title,desc):\n",
        "    num_feat=['price_enc', 'des_len_enc', 'tit_len_enc', 'pc_len_enc', 'des_char_enc',\n",
        "       'tit_char_enc', 'param_char_enc', 'punc_enc', 'avg_red_enc',\n",
        "       'avg_green_enc', 'avg_blue_enc', 'blur_enc']\n",
        "    cat_feat=['day', 'is_weekend', 'weekday','image_top_1','item_seq_number','region_enc', 'city_enc', 'pcn_enc', 'cn_enc',\n",
        "       'ut_enc', 'p1_enc', 'p2_enc', 'p3_enc']\n",
        "    model=load_model(\"/content/LSTM_with_Images.hdf5\",custom_objects={'rmse':rmse})\n",
        "    yp=model.predict([df[cat_feat],\n",
        "            df[num_feat],\n",
        "             desc,\n",
        "             title])\n",
        "    return yp\n",
        "\n",
        "\n",
        "\n",
        "  samp=load_sample(data)\n",
        "  fe_samp=feature_engg(samp)\n",
        "  enc_samp=data_encoding(fe_samp)\n",
        "  title_prepro=text_preprocess(fe_samp,'title')\n",
        "  des_prepro=text_preprocess(fe_samp,'description')\n",
        "  ttl_pad=text_generate(title_prepro,25000,7)\n",
        "  des_pad=text_generate(des_prepro,25000,250)\n",
        "  yp=predict(enc_samp,ttl_pad,des_pad)\n",
        "  return yp,samp"
      ],
      "metadata": {
        "id": "igmwiY40w56A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred,sa=final_pred(te_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSBW9qzY2dCz",
        "outputId": "7da89c08-6bc3-4a7c-d67e-10a6d059ea07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted Deal Probability Value is : {}\".format(ypred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrR3iCTFxgnd",
        "outputId": "361c2e56-4b12-4dd6-a090-1638e713da49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Deal Probability Value is : [[0.2613433]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_pred(data):\n",
        "  def select_data(df):\n",
        "    x=df[['region', 'city', 'parent_category_name',\n",
        "       'category_name', 'param_1', 'param_2', 'param_3', 'title',\n",
        "       'description', 'price', 'item_seq_number', 'activation_date',\n",
        "       'user_type', 'image', 'image_top_1', 'width',\n",
        "       'height', 'average_red', 'average_green', 'average_blue', 'image_size',\n",
        "       'img_blur']]\n",
        "    y=df['deal_probability']\n",
        "    return x,y\n",
        "\n",
        "  def data_split(x,y):\n",
        "    xtr,xcv,ytr,ycv=train_test_split(x,y,test_size=0.03,shuffle=False)\n",
        "    return xcv,ycv\n",
        "\n",
        "  def data_sample(x,y):\n",
        "    x=x.head(3)\n",
        "    y=y[:3]\n",
        "    return x,y\n",
        "\n",
        "  def feature_engg(df):\n",
        "    df['activation_date'] = pd.to_datetime(df['activation_date'], errors = 'coerce')\n",
        "    df['price']=np.log(df['price']+0.001)\n",
        "    df['day'] = df['activation_date'].dt.day\n",
        "    df['dayofweek_name'] = df['activation_date'].dt.day_name\n",
        "    df['is_weekend'] = np.where(df['dayofweek_name'].isin(['Sunday','Saturday']),1,0)\n",
        "    df['weekday'] = df['activation_date'].dt.weekday\n",
        "    df['description_len'] = df['description'].apply(lambda x : len(x.split()))\n",
        "    df['title_len'] = df['title'].apply(lambda x : len(x.split()))\n",
        "    df['param_combined'] = df.apply(lambda row: ' '.join([str(row['param_1']), str(row['param_2']),  str(row['param_3'])]), axis=1)\n",
        "    df['param_combined_len'] = df['param_combined'].apply(lambda x : len(x.split()))\n",
        "    df['description_char'] = df['description'].apply(len)\n",
        "    df['title_char'] = df['title'].apply(len)\n",
        "    df['param_char'] = df['param_combined'].apply(len)\n",
        "    df['punctuation_count'] = df['description'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) \n",
        "    return df\n",
        "\n",
        "  def data_encoding(df):\n",
        "    region_enc=pickle.load(open(\"/content/Region_Encoder.pkl\",\"rb\"))\n",
        "    df['region_enc']=region_enc.transform(df['region'].values.reshape(-1,1))\n",
        "    city_enc=pickle.load(open(\"/content/City_Encoder.pkl\",\"rb\"))\n",
        "    df['city_enc']=city_enc.transform(df['city'].values.reshape(-1,1))\n",
        "    pcn_enc=pickle.load(open(\"/content/parent_category_name_Encoder.pkl\",\"rb\"))\n",
        "    df['pcn_enc']=pcn_enc.transform(df['parent_category_name'].values.reshape(-1,1))\n",
        "    cn_enc=pickle.load(open(\"/content/category_name_Encoder.pkl\",\"rb\"))\n",
        "    df['cn_enc']=cn_enc.transform(df['category_name'].values.reshape(-1,1))\n",
        "    ut_enc=pickle.load(open(\"/content/user_type_Encoder.pkl\",\"rb\"))\n",
        "    df['ut_enc']=ut_enc.transform(df['user_type'].values.reshape(-1,1))\n",
        "    p1_enc=pickle.load(open(\"/content/param_1_Encoder.pkl\",\"rb\"))\n",
        "    df['p1_enc']=p1_enc.transform(df['param_1'].values.reshape(-1,1))\n",
        "    p2_enc=pickle.load(open(\"/content/param_2_Encoder.pkl\",\"rb\"))\n",
        "    df['p2_enc']=p2_enc.transform(df['param_2'].values.reshape(-1,1))\n",
        "    p3_enc=pickle.load(open(\"/content/param_3_Encoder.pkl\",\"rb\"))\n",
        "    df['p3_enc']=p3_enc.transform(df['param_3'].values.reshape(-1,1))\n",
        "    # Numerical Features\n",
        "  \n",
        "    price_enc=pickle.load(open(\"/content/Scaled_price.pkl\",\"rb\"))\n",
        "    df['price_enc']=price_enc.transform(df['price'].values.reshape(-1,1))\n",
        "    des_len_enc=pickle.load(open(\"/content/Scaled_description_len.pkl\",\"rb\"))\n",
        "    df['des_len_enc']=des_len_enc.transform(df['description_len'].values.reshape(-1,1))\n",
        "    tit_len_enc=pickle.load(open(\"/content/Scaled_title_len.pkl\",\"rb\"))\n",
        "    df['tit_len_enc']=tit_len_enc.transform(df['title_len'].values.reshape(-1,1))\n",
        "    pc_len_enc=pickle.load(open(\"/content/Scaled_param_combined_len.pkl\",\"rb\"))\n",
        "    df['pc_len_enc']=pc_len_enc.transform(df['param_combined_len'].values.reshape(-1,1))\n",
        "    des_char_enc=pickle.load(open(\"/content/Scaled_description_char.pkl\",\"rb\"))\n",
        "    df['des_char_enc']=des_char_enc.transform(df['description_char'].values.reshape(-1,1))\n",
        "    tit_char_enc=pickle.load(open(\"/content/Scaled_title_char.pkl\",\"rb\"))\n",
        "    df['tit_char_enc']=tit_char_enc.transform(df['title_char'].values.reshape(-1,1))\n",
        "    param_char_enc=pickle.load(open(\"/content/Scaled_param_char.pkl\",\"rb\"))\n",
        "    df['param_char_enc']=param_char_enc.transform(df['param_char'].values.reshape(-1,1))\n",
        "    punc_enc=pickle.load(open(\"/content/Scaled_punctuation_count.pkl\",\"rb\"))\n",
        "    df['punc_enc']=punc_enc.transform(df['punctuation_count'].values.reshape(-1,1))\n",
        "    avg_red_enc=pickle.load(open(\"/content/Scaled_average_red.pkl\",\"rb\"))\n",
        "    df['avg_red_enc']=avg_red_enc.transform(df['average_red'].values.reshape(-1,1))\n",
        "    avg_green_enc=pickle.load(open(\"/content/Scaled_average_green.pkl\",\"rb\"))\n",
        "    df['avg_green_enc']=avg_green_enc.transform(df['average_green'].values.reshape(-1,1))\n",
        "    avg_green_enc=pickle.load(open(\"/content/Scaled_average_green.pkl\",\"rb\"))\n",
        "    df['avg_green_enc']=avg_green_enc.transform(df['average_green'].values.reshape(-1,1))\n",
        "    avg_blue_enc=pickle.load(open(\"/content/Scaled_average_blue.pkl\",\"rb\"))\n",
        "    df['avg_blue_enc']=avg_blue_enc.transform(df['average_blue'].values.reshape(-1,1))\n",
        "    blur_enc=pickle.load(open(\"/content/Scaled_img_blur.pkl\",\"rb\"))\n",
        "    df['blur_enc']=blur_enc.transform(df['img_blur'].values.reshape(-1,1))\n",
        "    return df\n",
        "\n",
        "  def text_preprocess(df,col):\n",
        "    corpus=[]\n",
        "    for i in tqdm(range(0,len(df[col]))):\n",
        "      review=re.sub(r'[^\\w\\s]',\" \",df[col].values[i])\n",
        "      review=re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", review)\n",
        "      review = re.sub(r\"http\\S+\", \" \", review)\n",
        "      review = BeautifulSoup(review, 'lxml').get_text()\n",
        "      review = re.sub('_',' ',review)\n",
        "      review = ' '.join(text_lemmatizer(review, langdata))\n",
        "      review = ' '.join([word for word in review.split() if not word in stopwords.words('russian')])\n",
        "      review = ' '.join([word for word in review.split() if not word in (punctuation)])\n",
        "      review = review.lower()\n",
        "      corpus.append(review)\n",
        "    return corpus\n",
        "\n",
        "  def text_generate(corpus,voc_size,maxleng):\n",
        "    onehot_repr=[one_hot(words,voc_size)for words in corpus]\n",
        "    embedded_docs=pad_sequences(onehot_repr,padding='post',maxlen=maxleng)\n",
        "    return embedded_docs\n",
        "\n",
        "\n",
        "#'price_enc', 'des_len_enc', 'tit_len_enc', 'pc_len_enc', 'des_char_enc',\n",
        "#       'tit_char_enc', 'param_char_enc', 'punc_enc', 'avg_red_enc',\n",
        "#       'avg_green_enc', 'blur_enc'\n",
        "\n",
        "  def predict(xcv,ycv,title,desc):\n",
        "    num_feat=['price_enc', 'des_len_enc', 'tit_len_enc', 'pc_len_enc', 'des_char_enc',\n",
        "       'tit_char_enc', 'param_char_enc', 'punc_enc', 'avg_red_enc',\n",
        "       'avg_green_enc', 'avg_blue_enc', 'blur_enc']\n",
        "    cat_feat=['day', 'is_weekend', 'weekday','image_top_1','item_seq_number','region_enc', 'city_enc', 'pcn_enc', 'cn_enc',\n",
        "       'ut_enc', 'p1_enc', 'p2_enc', 'p3_enc']\n",
        "    model=load_model(\"/content/LSTM_with_Images.hdf5\",custom_objects={'rmse':rmse})\n",
        "    ypred=model.predict([xcv[cat_feat],\n",
        "            xcv[num_feat],\n",
        "             desc,\n",
        "             title])\n",
        "    return ycv,ypred\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  x,y=select_data(data)\n",
        "  xcv,ycv=data_split(x,y)\n",
        "  xcv,ycv=data_sample(xcv,ycv)\n",
        "  xcv=feature_engg(xcv)\n",
        "  xcv=data_encoding(xcv)\n",
        "  title_prepro=text_preprocess(xcv,'title')\n",
        "  des_prepro=text_preprocess(xcv,'description')\n",
        "  ttl_pad=text_generate(title_prepro,25000,7)\n",
        "  des_pad=text_generate(des_prepro,25000,250)\n",
        "  ycv,ypred=predict(xcv,ycv,ttl_pad,des_pad)\n",
        "  return ycv,ypred"
      ],
      "metadata": {
        "id": "LMx2utWYzi04"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yact,ypred=final_pred(tr_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otWftiyn17Ob",
        "outputId": "076919cf-1889-4d2a-badb-7df5851b6fd1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "100%|██████████| 3/3 [00:00<00:00, 651.76it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 422.80it/s]\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f20c4c5a4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(yact.shape,ypred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dKptc6EBFf0",
        "outputId": "39ba532b-c500-4242-eb0f-397bda4d1059"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,) (3, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ya=np.array(yact)"
      ],
      "metadata": {
        "id": "9keNGkv1BySk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ya.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aFHuAUhB6ao",
        "outputId": "84f1a7e8-d189-4edd-a556-81440d9879a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ya=ya.reshape(-1,1)\n",
        "ya.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoAU2hJGB8yG",
        "outputId": "cc0d0bff-43a0-44d1-97e4-9975de24dda4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rmse(ya,ypred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cyZs1lO_jib",
        "outputId": "d0b36b47-db2a-4ed1-b317-1642a1f9ef1d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.10544068104368096, shape=(), dtype=float64)\n"
          ]
        }
      ]
    }
  ]
}